{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import MDP\n",
    "import RL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Construct simple MDP as described in Lecture 2a Slides 13-14'''\n",
    "T = np.array([[[0.5,0.5,0,0],[0,1,0,0],[0.5,0.5,0,0],[0,1,0,0]],[[1,0,0,0],[0.5,0,0,0.5],[0.5,0,0.5,0],[0,0,0.5,0.5]]])\n",
    "R = np.array([[0,0,10,10],[0,0,10,10]])\n",
    "discount = 0.9\n",
    "mdp = MDP.MDP(T,R,discount)\n",
    "rlProblem = RL2.RL2(mdp,np.random.normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 0: REINFORCE for Company Env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows results for reinforce algorithm for the company environment. The argument optionlr=2 is a learning rate option which selects different learning rate cool down periords. This learning rate option is selected based as it performed the best in Maze environment (as can be seen from the graph).\n",
    "The learning rate:<br>\n",
    "episode number (0-59): 0.004<br>\n",
    "episode number (60-119): 0.003<br>\n",
    "episode number (120-179): 0.002<br>\n",
    "episode number (180:inf): 0.001<br>\n",
    "\n",
    "The results match the results we obtain from other approximate and deterministic methods like Q-Learning, value iteration and policy iteration. The stochastic policy in REINFORCE algorithm still shows a small possibility of choosing bad actions, however it could be because of numerical approximations in python. The policy is derived by applying softmax over all actions' policy parameters for a given state. This non-linear and strictly increasing transformation of the policy parameters to generate policy can be used to evaluate the policy parameter results as a value function. Higher value of policy parameters generate a higher probability of taking that action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "REINFORCE results\n",
      "[[   1.08779021 -107.64104214  -72.11244365 -118.98323027]\n",
      " [-190.65790366    1.39528767    1.55156471    1.55799232]]\n",
      "[[  1.00000000e+00   4.42714387e-48   1.01889083e-32   4.46284897e-53]\n",
      " [  5.31989696e-84   1.00000000e+00   1.00000000e+00   1.00000000e+00]]\n",
      "last 10 episode rewards: [ 61.83802954  41.71160069  36.4315967    3.13309967   4.96709418\n",
      "  40.15740254  66.25189017  10.63413635  33.92608282  54.1812685 ]\n"
     ]
    }
   ],
   "source": [
    "# Test REINFORCE\n",
    "[policyParams,policy] = rlProblem.reinforce(\n",
    "    s0=0,initialPolicyParams=np.random.rand(mdp.nActions,mdp.nStates),\n",
    "    nEpisodes=1000,nSteps=100,optionlr=2)\n",
    "print (\"\\nREINFORCE results\")\n",
    "print (policyParams)\n",
    "print (policy)\n",
    "print (\"last 10 episode rewards: {}\".format(rlProblem.get_reinforce_cumulative_reward()[-10:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 0: Model Based RL for Company Env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following results show that the Model Based RL computes the correct policy in the Company environment (based on the results from Value Iteration). Model based RL algorithm will learn a model (transition matrix) of the environment by generating an expectation over the samples accumulated. The value function generated by model based RL is $[ 30.35689505,  37.17731415,  42.57629584,  52.68348209]$ and the value function generated for Company environment is $[ 31.58404185,  38.60295392,  44.0231138 ,  54.2005363 ]$. We see that the values are comparable and the differences are because model based RL didn't have the real transition matrix and thus was trying to evaluate the best policy on an estimate of the transition matrix (generated by sampling (s,a,s') from the environment). We achieve a final deterministic policy which is exactly the same as other approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model-based RL results\n",
      "[ 30.35689505  37.17731415  42.57629584  52.68348209]\n",
      "[0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Test model-based RL\n",
    "[V,policy] = rlProblem.modelBasedRL(s0=0,defaultT=np.ones([mdp.nActions,mdp.nStates,mdp.nStates])/mdp.nStates,initialR=np.zeros([mdp.nActions,mdp.nStates]),nEpisodes=100,nSteps=100,epsilon=0.05)\n",
    "print (\"\\nmodel-based RL results\")\n",
    "print (V)\n",
    "print (policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
